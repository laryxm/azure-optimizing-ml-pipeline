{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing an ML Pipeline in Azure\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "   * [Overview](#Overview)\n",
    "   * [Summary](#Summary)\n",
    "   * [Scikit-learn Pipeline](#Scikit-learn-Pipeline)\n",
    "   * [AutoML](#AutoML)\n",
    "   * [Pipeline comparison](#Pipeline-comparison)\n",
    "   * [Future work](#Future-work)\n",
    "   * [Proof of cluster clean up](#Proof-of-cluster-clean-up)\n",
    "   * [Citation](#Citation)\n",
    "   * [References](#References)\n",
    "   \n",
    "\n",
    "## Overview\n",
    "This project is part of the Udacity Azure ML Nanodegree.  \n",
    "In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. This model is optimized using hyperparameter tunning artifacts from HyperDrive. The best model is then compared to another model generated by Azure AutoML run.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "### The data set\n",
    "The \"Bank Marketing\" data set contains information about direct marketing campaigns of a Portuguese banking institution.  \n",
    "\n",
    "In this project we seek to predict if the client would subscribe a bank term deposit ('yes') or not ('no'). Finding this answer we can help the financial institution choose better strategies for a greater effectiveness for future marketing campaigns. \n",
    "\n",
    "### The best model  \n",
    "\n",
    "The best performing model was a Voting Ensemble model generated by an AutoML job. It was used nine models using   XGBoostClassifier and LightGBMalgorithms algorithm. \n",
    "\n",
    "The primary metric used was the accuracy and it achieved 0.9167.  \n",
    "However, since it is an unbalanced data set, it is interesting to analyse other metrics results:  \n",
    "- f1_score_micro: 0.9167\n",
    "- precision_score_weighted: 0.9107\n",
    "- AUC_weighted: 0.9477\n",
    "\n",
    "\n",
    "## Scikit-learn Pipeline\n",
    "\n",
    "### Data Analysis\n",
    "\n",
    "The data set has information about the clients 10,000 clients. There are some demographic attributes like age, type of job, marital status and education. And attributes about their financial profile like and the related campaign: if they have housing loan, credit in default, personal loan and other.  \n",
    "\n",
    "The target variable **y** answer if the client has subscribed a term deposit.\n",
    "\n",
    "### Preprocessing data\n",
    "\n",
    "The Scikit-learn pipeline obtains the provided data from the provided URL. Following data download, a number of data cleaning steps are carried out including:\n",
    "\n",
    "1. Removing NAs from the dataset.\n",
    "2. One-hot encoding job titles, contact, and education variables.\n",
    "3. Encoding a number of other categorical variables.\n",
    "4. Encoding months of the year.\n",
    "5. Encoding the target variable.\n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "Before training the model, the data was split into a training and testing set. It was set a test set size of 30% of total entries. The classification algorithm used was **Logistic Regression**, which has parameters like the Regularization Strength **C** and **maximum number of iterations** to be set.\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Azure's Hyperdrive service was used for hyperparameter tuning with the following key elements:\n",
    "\n",
    "#### Parameter sampling\n",
    "\n",
    "It was used a Random Parameter Sampling. \n",
    "Grid sampling does a simple grid search over all possible values which can be very expensive.\n",
    "Random Parameter Sampling also can be less expensive to fit a model and it can make full use of all available nodes compared to Bayesian Parameter Sampling.\n",
    "When using Bayesian sampling, the number of concurrent runs has an impact on the effectiveness of the tuning process  because some runs can start without fully benefiting from runs that are still running.\n",
    "\n",
    "#### Early stopping policy  \n",
    "\n",
    "Automatically end poorly performing runs with an early termination policy. \n",
    "Choosing an Early termination improves computational efficiency. In this project was selected the **Bandit Policy** stopping policy because it allows a higher saving. Choosing an Early stopping policy is a good option because we don't want the hyperparameter tuning service let all training runs execute to completion. \n",
    "\n",
    "The best model parameters here were a C value of 5.00 and a maximum number of iterations value of 147. The model's accuracy was 91.54% and the SKLearn pipeline experiment was executed in a compute cluster.\n",
    "\n",
    "\n",
    "\n",
    "## AutoML  \n",
    "\n",
    "The autoML pipeline used the same steps before starting the training process: The data were cleaned, preprocessed and splited into train and test sets.  \n",
    "\n",
    "The autoML experiment was processed locally and the pipeline tested RandomForest, XGBoostClassifier, GradientBoosting, LogisticRegression, ExtremeRandomTrees, LightGBM.  \n",
    "\n",
    "The best model selected by autoML was a voting ensemble with the accuracy as primary metric, obtaining of 91.67%. It was selected nine models using XGBoostClassifier and LightGBMalgorithms algorithms.  \n",
    "\n",
    "\n",
    "## Pipeline comparison\n",
    "**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**\n",
    "\n",
    "The two models performed very similarly in terms of accuracy, with the hyperdive model achieving 91.54% accuracy and the autoML model achieving 91.67% accuracy. However, this study showed me that autoML process can bring excellent results with less effort and had a great performance in other metrics as well such as precision, recall and AUC which is more suitable for unbalanced data like in this case. \n",
    "\n",
    "\n",
    "## Future work\n",
    "**What are some areas of improvement for future experiments? Why might these improvements help the model?**\n",
    "\n",
    "In the future we can improve the model by: \n",
    "\n",
    "- Exploring feature engineering process,\n",
    "- Testing scaling methods for the continuous variables,\n",
    "- Testing oversampling methods such as SMOTE and ADASYN,\n",
    "- Running AutoML for more time (increase the \"experiment_timeout_minutes\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
